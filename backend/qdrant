import os
from dotenv import load_dotenv
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.http import models

load_dotenv()
# # üîë Clients
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
qdrant = QdrantClient(
    url=os.environ["QDRANT_URL"],
    api_key=os.environ["QDRANT_API_KEY"]
)

# qdrant = QdrantClient(
#     url=os.environ["JOHN_URL"],
#     api_key=os.environ["JOHN_API_KEY"]
# )

COLLECTION_NAME2 = "chicorylane"

# 1Ô∏è‚É£ Create Qdrant collection (only once)
# qdrant.recreate_collection(
#     collection_name=COLLECTION_NAME2,
#     vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
# )

# 2Ô∏è‚É£ Ingest sample documents
# docs = [
#     {"id": 1, "text": "Qdrant is an open-source vector database designed to handle large-scale vector embeddings for machine learning applications."},
# ]

# vectors, payloads, ids = [], [], []
# for d in docs:
#     emb = client.embeddings.create(
#         model="text-embedding-3-small",
#         input=d["text"]
#     ).data[0].embedding

#     vectors.append(emb)
#     payloads.append({"text": d["text"]})
#     ids.append(d["id"])

# qdrant.upsert(
#     collection_name=COLLECTION_NAME2,
#     points=models.Batch(ids=ids, vectors=vectors, payloads=payloads)
# )




# query = "What are vernal pools?"
# # # 3Ô∏è‚É£ Retrieve Context Based on Query
# #Pulling information from Qdrant based on user query for query augmentation
# query_emb = client.embeddings.create(
#     model="text-embedding-3-small",
#     input=query
# ).data[0].embedding

# search_result = qdrant.query_points(
#     collection_name=COLLECTION_NAME2,
#     query=query_emb,
#     limit=3
# )

# context = "\n".join([hit.payload["text"] for hit in search_result if hit.payload and "text" in hit.payload])
# print("üîç Retrieved context:\n", context)

# # 4Ô∏è‚É£ Ask ChatGPT given context of model
# # augmentation step, can edit call to GPT as needed if you want more or less context 
# chat_response = client.chat.completions.create(
#     model="gpt-4.1-mini",
#     messages=[
#         {"role": "system", "content": "You are a helpful assistant. Use the provided context to answer."},
#         {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
#     ]
# )

# print("\nüí° ChatGPT Answer:\n", chat_response.choices[0].message.content)
# 1Ô∏è‚É£ Embed the user query
query = "Why is land a commodity?"
query_emb = client.embeddings.create(
    model="text-embedding-3-small",
    input=query
).data[0].embedding

# 2Ô∏è‚É£ Retrieve relevant points from Qdrant (v2 syntax)
search_result = qdrant.query_points(
    collection_name=COLLECTION_NAME2,
    query=query_emb,
    limit=3
)

# 3Ô∏è‚É£ Extract context text properly from Qdrant v2 output
context_chunks = []
for point in search_result.points:
    payload = point.payload or {}
    if "text" in payload:
        context_chunks.append(payload["text"])

context = "\n".join(context_chunks)
print("üîç Retrieved context:\n", context)

# 4Ô∏è‚É£ Ask OpenAI using provided context
chat_response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant. Use the provided context to answer the user's query."},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
    ]
)

answer = chat_response.choices[0].message.content

print("\nüí° ChatGPT Answer:\n", answer)
